\documentclass{article}
\usepackage{tabularx}
\usepackage{tikz}
\usepackage[left=1in,right=1in,top=1in,bottom=1in]{geometry} 
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{mathtools}
\usepackage{siunitx}
\usepackage{booktabs}
\usepackage{bbm}
\usepackage{calc}
\usepackage{boxedminipage}
\usepackage{capt-of}
\usepackage{float}
\usepackage{subcaption}

\usepackage{makecell}
\usepackage{changepage}
\usetikzlibrary{bayesnet, shapes,shapes.multipart}

\DeclareMathOperator*{\argmax}{argmax}  

\setlength{\parindent}{0mm}
\newtheorem{theorem}{Theorem}
\newtheorem{proof}{Proof}

\begin{document}

\title{The Random Walk of Distraction}
\author{Andrew T. Smith}
\date{\today}
\maketitle

\section{Introduction}
The events that distract us seem to occur randomly, including both external stimuli and internal thoughts, such as like a song stuck in one's head.  sgasrg This can be modeled as a Poisson process, but we cannot observe the exact time a user becomes distracted, only periodically inquire about what they are experiencing.  Second, the "sticky" nature of distraction means that a user's estimate of how many recent events they experienced that \emph{could} have pulled them away from their task is only reliable if it is 0 or 1.  Truly distracting events are assumed drive the user into a terminal state, in this sense. One can be distracted \emph{from} other distractions but returning to work is a dicier prospect, and for this paper, assumed not to happen.

\section{Distraction Model}
The distraction model learns from data generated by a sequence of $N$ trials.  Let the observation of trial $i$ be the pair $(A_i, T_i)$, where $A_i$ is the user's binary response at the end and $T_i$ is the duration of the trial.  During each trial the user will work on other tasks, and during this time they will be confronted with an unknown number $D_i$ of irresistible distractions.  At the trial's conclusion, they are asked if they are either in a distracted state or have remained working.  Their answer, $A_i$ (where $A_i=1$ means ``yes, distracted", etc.) is used to update the model parameters $\alpha_i$ and $\beta_i$, which define the gamma distribution of the latent random variable  $\lambda_i$, the expected number of distractions during time $T_i$.  The actual number of distracting events, $D_i$, has a Poisson distribution with parameter $\lambda_i$.
\bigskip
\renewcommand{\arraystretch}{1.5}
\newcommand{\poisson}{{\rm Pr}}
\newcommand{\exponential}{{\rm Exp}}


{\centering
\begin{tabularx}{\linewidth-30pt}{r c X}
	\toprule
		\makecell{Param.\\} & \makecell{Definition} &Description\\
	\midrule
		$\alpha_i$, $\beta_i$ &  & model parameters for trial $i$\\
		$\lambda_i$& $\frac{\beta^{\alpha}}{\Gamma(\alpha)}\lambda^{\alpha-1}e^{-\beta\lambda}$&rate of distraction, gamma distribution, $\Gamma_{\alpha_i,\beta_i}(\lambda_i),$ for trial $i$\\
		$D_i$& $\frac{\lambda^D}{D!} e^{-\lambda}$ & distraction count of trial $i$, i.e. the Poisson distribution $\poisson_{\lambda_i}(D_i)$\\
		$A_i$ &  & user response $A_i=\mathbbm{1}_{D_i>0}$, i.e.\ $1$ if distracted, otherwise $0$\\
	\bottomrule
\end{tabularx}
\bigskip\\
\begin{minipage}{\linewidth-40pt}
	\centering
	\tikz{% nodes-
	
		\node[latent, draw] (gamma) {$\alpha, \beta$}; 				
		\node[latent,right=of gamma] (lambda) {$\lambda$}; 

		\node[latent,right=of lambda] (D) {$D$}; 				
		\node[obs, right=of D,  draw] (A) {$A$};
		
		\edge {lambda} {D}; 
		\edge {D} {A}; 
		\edge {gamma}{lambda}; 

		
	}
	\captionof{figure}{Distraction model:  observed variables are shaded, latent unshaded.  (Parameters without distributions have.}
\end{minipage}
}
\bigskip

Trial durations $T(i)$ are set to the wait time for the probability that at least one distraction has occurred to exceed the (user-set) threshold $p$, given the \emph{maximum a posteriori} estimate of $\lambda_i$,  i.e. $P(D_i\geq 1|\lambda_i) \geq p$.  Since $D$ has a Poisson distribution, the expected wait times between distracting events has an exponential distribution with the same parameter $\lambda$:
\begin{align}
  P(t<T)&=F_{\lambda}(t)=1-e^{-\lambda  t}\nonumber\\
  T(i)\coloneqq& F_{\lambda}^{-1}\label{waittime}(p)
\end{align}
where $F_{\lambda}$ is the exponential CDF.  

The user can also decide to end the time period earlier, resulting in an $(A, T)$ observation with a shorter $T$.
\subsection{Learning with Monte Carlo}



\subsubsection{Factoring the joint distribution}
The graphical model encodes conditional independence relationships that simplify factoring the joint distribution (first step) and using the fact that the deterministic variables have $p=1$ simplifies it further (2nd step):
\begin{align}
p(L, D, T, \lambda, \alpha, \beta) &= p(L| D, T, \lambda, \alpha, \beta)p(D| T, \lambda, \alpha, \beta)p(T| \lambda, \alpha, \beta)p(| \lambda| \alpha, \beta) \nonumber\\
&= p(T|P)p(\lambda|\alpha, \beta)p(D|\lambda, T)p(L|D)\nonumber\\
&=p(\lambda|\alpha, \beta)p(D|\lambda, T)\label{joint},
\end{align}
The generative process starts with $\lambda$ being sampled from the gamma distribution defined by the current values of $\alpha$ and $\beta$. Then the trial duration $T$ is calculated from $\lambda$ and the user setting $p$. $D$ is sampled from the Poisson distribution ??? with rate $\lambda/T$. Finally, the user reports their answer $A$, whether $D>0$ or not.



BROKEN AFTER HERE



Given the rate of user distractions $\lambda$, the number of distractions $D(i)$ is a random variable with a Poisson distribution
\begin{equation}
  P(D=k|\lambda,t) = \frac{ (t\lambda)^k e^{-t\lambda}}{k!}% {{}\label{exp_dist}
\end{equation}
where $F_\lambda$ is the exponential CDF with rate parameter $\lambda$.  Therefore the time required for the probability of the next distraction to exceed $p=P(t;\lambda)$ is the inverse CDF:
\begin{align}
  t_\lambda(p)& = F_\lambda^{-1}(p)\nonumber\\
  & = \frac{\ln(\lambda) -\ln P(t;\lambda) }{\lambda}\label{inv_dist}
\end{align}


\section{Interrupting a distracted user}
This is an outline of an app to model a user's distraction process by using timed alarms to prompt the user to input their mental state.
\subsection{Basic usage}
\begin{enumerate}
\item The user sets an "alarm" by setting a threshold $P(t;\lambda)$ or, equivalently, the expected wait time for that threshold to be exceeded $t_\lambda(p)$.
\item After the expected wait time has elapsed, the alarm is activated.
\item The alarm should have three different buttons that deactivate and reset it:
\begin{itemize}
\item Red - User was distracted and should have been interrupted sooner.
\item Yellow - User was distracted but has only recently become so.
\item Green - User was not distracted and should not have been interrupted so soon.
\end{itemize}
\item The app adjusts the alarm period based on which button was pressed, and repeats back to step 1.
\item If the user presses the buttons early, the response is the same.
\end{enumerate}
\subsection{App features}
\begin{itemize}
\item The alarm should be timed so as not to be too frequent that it \emph{itself} interrupts the user's work, while also not too infrequent that the user wastes much time in a distracted state.
\item The alarm timing should be well-calibrated, in the sense that if the user selects a threshold probability $p$, then the proportion of times the alarm find the user in a distracted state should approach $p$.  This is what the proportion of clicks in \{Red, Green\} that are Red (user is distracted) approaches until Yellow is clicked, because:
\item A Yellow click indicates directly that the alarm is calibrated and the rate should not be adjusted.
\item The rate should be calculated from as many of the most recent clicks as possible, but is constrained by Yellow clicks.  Because the user did not desire adaptation at this point, clicking Red / Green after a Yellow means the former rate $\lambda$ is no longer valid, but a reasonable prior probability $P(\lambda)$.


\item Adaptation can be optimal:  the Red and Green responses should allow the alarm timing to approach the user's desired rate as quickly as possible, or adapt as responsively as possible as that rate changes over time.
\end{itemize}

\section{Optimal Adaptation - Bayesian updates}
The probability threshold is set by the user, so the app must adjust an internal estimate of the rate parameter $\lambda$ to raise the alarm at the desired time.  Since the user's Yellow clicks indicate optimality, we only update the model based on data after the most recent Yellow click.

Let:
\begin{itemize}
\item $L(t)\in \{\text{Red},\text{Yellow},\text{Green}\}$ be the sequence of letters indicating the button pressed at time $t$ for $0 \leq t \leq T$, e.g. $L=(..., R, G, Y, Y, R, G)$
\item $D(T)$ be the duration of the time period preceeding the button click at time $t$ (e.g. in seconds), and
\item $A(t)$ indicate the state of the alarm $A(t) \in \{0=$ silence, $1=$ alarm$\}$.
\end{itemize} 
Given the most recent user interaction from time $S$ to time $T$,  $D=\{(d_t, l_t, a_t) : S \leq t \leq T , L(S)=Y or S=0\}$, what is the best estimate of the rate $\lambda$?


The Bayesian update model consists of:
\begin{align}
  P(\lambda|d, l) &= \Gamma(x) P(d, l|\lambda)\nonumber\\
  P(\lambda|d, l)
\end{align}
%\begin{equation}
 %\text{the posterior distribution of $\lambda$ given the evidence,}\\
 %P(d, l|\lambda) \text{the likelihood of the evidence given the model, i.e.\ the distraction model, and}\\
%P(d,l) \text{the normalized distribution of the data.}


and  uses Bayes' rule to get the new distribution of$\lambda$:








%\caption{Do not forget!
%Make it explicit enough that readers
%can figure out what you are doing.}
%\end{figure}









\section{old}

Given the User's history with the alarm clock, a sequence of obersvations, pairs of time durations and the user's responses to the alarms at the end of each one, $O_i = (d_i, r_i)$, where $r_i \in \{ \text{red},\text{yellow}, \text{green}\}$ and $0\leq i\leq N$, the optimal rate  is defined as the maximum \emph{a posteriori} estimate of $\lambda$.

The conjugate prior of the exponential distribution with parameter $\lambda$ is:
$$
 \pi(\lambda;k, \theta) = \frac{1}{\Gamma(k)\theta^k}\lambda^{k-1} e^\frac{\lambda}{\theta} 
$$

where $1/\theta$ is $\beta$,  the "rate paramter" of the Gamma distribution.

Given new observation at time $n+1$:

$$
P(\theta|O_{n+1}) = \frac{P(O_{n+1}|\theta) * P(\theta)}{P(O_{n+1})}
$$
Which suggests the update rule:
\begin{align}\label{eqn}
\theta_{n+1} &= \argmax_{\theta}   \frac{P(O_{n+1}|\theta_n) * P(\theta_n)}{P(O_{n+1})}\nonumber\\
 &= \argmax_{\theta} \text{Exp}(O_{n+1}|\theta_n)  \\ 
\end{align}
\section{Future work}
The state of the alarm is not used in the update.  It is determined only by the user's threshold setting and the current probability estimate (which is defined by the current rate estimate $\lambda$ and time $t$), and its only purpose is to get the user to interact with the app.  This raises two issues:
\begin{enumerate}
\item If user presses Red/Green when the alarm is not ringing, the durations $D(t)$ in the trainable suffix will be biased, expected to be less than the durations resulting from each button press being immediately after an alarm started.  The effects of this on the learning are unclear.


\item Beyond learning the distraction rate $\lambda$, yt is also possible to learn what makes a user more / less likely to interact with the app, an interaction rate $\xi$.  Like many active learning problems, this may lead to evil.
\end{enumerate}
\end{document}
